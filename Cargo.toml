[package]
name = "aha"
version = "0.1.1"
edition = "2024"
repository = "https://github.com/jhqxxx/aha"
license = "Apache-2.0"
description = "aha model inference library, now supports Qwen2.5VL, MiniCPM4, VoxCPM, and Qwen3VL"

[dependencies]
candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.9.1", tag = "0.9.1"}
candle-nn = { git = "https://github.com/huggingface/candle.git", version = "0.9.1", tag = "0.9.1"}
candle-transformers = { git = "https://github.com/huggingface/candle.git", version = "0.9.1", tag = "0.9.1"}
candle-flash-attn = { git = "https://github.com/huggingface/candle.git", version = "0.9.1", tag = "0.9.1", optional = true }
serde = "1.0.226"
serde_json = "1.0.145"
anyhow = "1.0.100"
ffmpeg-next = "8.0.0"
image = "0.25.8"
reqwest = { version = "0.12.23", features = ["blocking"] }
base64 = "0.22.1"
num = "0.4.3"
minijinja = "2.12.0"
tokenizers = "0.22.1"
openai_dive = { git = "https://github.com/jhqxxx/openai-client.git", version = "1.3.3", tag = "1.3.3", features = ["stream"]}
uuid = { version = "1.18.1", features = ["v4"]}
chrono = "0.4.42"
rocket = "0.5.1"
tokio = "1.47.1"
hound = "3.5.1"

[features]
flash-attn=["candle-flash-attn"]
cuda=["candle-nn/cuda", "candle-core/cuda", "candle-transformers/cuda"]
